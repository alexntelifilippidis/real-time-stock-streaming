# ğŸ“ˆ Real-Time Stock Streaming Pipeline

[![Python](https://img.shields.io/badge/Python-3.8%2B-blue.svg)](https://www.python.org/)
[![Apache Kafka](https://img.shields.io/badge/Apache%20Kafka-3.0%2B-black.svg)](https://kafka.apache.org/)
[![Apache Spark](https://img.shields.io/badge/Apache%20Spark-3.3%2B-orange.svg)](https://spark.apache.org/)
[![Docker](https://img.shields.io/badge/Podman-4.0%2B-892CA0.svg)](https://www.podman.io/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

A production-ready, real-time data streaming pipeline that demonstrates end-to-end data engineering using **Apache Kafka** and **Apache Spark Structured Streaming**. This project simulates live stock market data, processes it in real-time, and provides actionable analytics.

## ğŸ¯ Key Features

- âš¡ **Real-time Processing**: Sub-second latency streaming with Kafka and Spark
- ğŸ“Š **Scalable Architecture**: Horizontally scalable components using Podman
- ğŸ”„ **Fault Tolerance**: Automatic recovery and checkpointing
- ğŸ“ˆ **Live Analytics**: Real-time aggregations and windowing operations
- ğŸ³ **Easy Deployment**: Full Podman Compose setup for local development
- ğŸ“ **Production Ready**: Logging, monitoring, and error handling included

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Data Generator â”‚ â”€â”€â”€â–¶ â”‚  Apache Kafka   â”‚ â”€â”€â”€â–¶ â”‚  Spark Stream   â”‚
â”‚  (Python)       â”‚      â”‚  (Topic: stock) â”‚      â”‚  (PySpark)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                            â”‚
                                                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Dashboard     â”‚ â—€â”€â”€â”€ â”‚   PostgreSQL    â”‚ â—€â”€â”€â”€ â”‚  Aggregations   â”‚
â”‚ (Streamlit)     â”‚      â”‚   (Storage)     â”‚      â”‚  (Processing)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Data Flow:**
1. ğŸ² **Producer** generates realistic stock price data (AAPL, GOOGL, MSFT, AMZN, etc.)
2. ğŸ“¨ **Kafka** ingests and buffers messages in the `stock_prices` topic
3. âš¡ **Spark Structured Streaming** consumes and processes data with windowed aggregations
4. ğŸ’¾ **PostgreSQL** stores processed results for persistence
5. ğŸ“Š **Streamlit Dashboard** visualizes real-time trends and analytics

## ğŸ› ï¸ Tech Stack

| Layer | Technology | Purpose |
|-------|-----------|---------|
| ğŸ”„ **Streaming** | Apache Kafka | High-throughput message broker |
| âš¡ **Processing** | Apache Spark (Structured Streaming) | Real-time stream processing |
| ğŸ—ƒï¸ **Storage** | PostgreSQL | Relational database for analytics |
| ğŸ **Language** | Python 3.8+ | Main implementation language |
| ğŸ“¦ **Libraries** | PySpark, kafka-python, pandas | Data processing |
| ğŸ“Š **Visualization** | Streamlit | Interactive dashboards |
| ğŸ³ **Infrastructure** | Docker & Podman Compose | Containerization |
| ğŸ” **Monitoring** | Prometheus + Grafana (planned) | Metrics and observability |

## ğŸ“ Project Structure

```
real-time-stock-streaming/
â”‚
â”œâ”€â”€ README.md                      # This file
â”œâ”€â”€ LICENSE                        # MIT License
â”œâ”€â”€ .gitignore                     # Git ignore rules
â”œâ”€â”€ podman-compose.yml             # Container services configuration
â”œâ”€â”€ pyproject.toml               # Python dependencies
â”‚
â”œâ”€â”€ kafka/                         # Kafka producer/consumer
â”‚   â”œâ”€â”€ producer.py               # Stock data generator
â”‚   â”œâ”€â”€ consumer_test.py          # Test consumer for validation
â”‚   â””â”€â”€ config.py                 # Kafka configuration
â”‚
â”œâ”€â”€ spark/                         # Spark streaming jobs
â”‚   â”œâ”€â”€ spark_streaming.py        # Main streaming application
â”‚   â”œâ”€â”€ aggregations.py           # Aggregation logic
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ schema.py             # Data schemas
â”‚       â””â”€â”€ helpers.py            # Utility functions
â”‚
â”œâ”€â”€ database/                      # Database scripts
â”‚   â”œâ”€â”€ init.sql                  # PostgreSQL initialization
â”‚   â””â”€â”€ queries.sql               # Sample queries
â”‚
â”œâ”€â”€ dashboard/                     # Streamlit dashboard
â”‚   â”œâ”€â”€ app.py                    # Main dashboard app
â”‚   â””â”€â”€ components/               # UI components
â”‚
â”œâ”€â”€ data/                          # Sample and test data
â”‚   â””â”€â”€ sample_stock_data.json    # Example data
â”‚
â”œâ”€â”€ notebooks/                     # Jupyter notebooks
â”‚   â””â”€â”€ exploration.ipynb         # Data exploration
â”‚
â””â”€â”€ tests/                         # Unit tests
    â”œâ”€â”€ test_producer.py
    â””â”€â”€ test_streaming.py
```

## ğŸš€ Quick Start

### Prerequisites

- **Podman** 4.0+ and **Podman Compose** 2.0+
- **Python** 3.8 or higher
- **Git** for cloning the repository
- At least 4GB RAM available for containers

### Installation

1. **Clone the repository**
   ```bash
   git clone https://github.com/your-username/real-time-stock-streaming.git
   cd real-time-stock-streaming
   ```

2. **Start the infrastructure** (Kafka, Zookeeper, PostgreSQL)
   ```bash
   podman-compose up -d
   ```
   
   Wait for services to be healthy (~30 seconds):
   ```bash
   podman-compose ps
   ```

3. **Create Python virtual environment**
   ```bash
   python -m venv venv
   source venv/bin/activate  # On macOS/Linux
   # venv\Scripts\activate   # On Windows
   ```

4. **Install dependencies**
   ```bash
   uv sync
   ```

5. **Initialize the database**
   ```bash
   podman-compose exec postgres psql -U postgres -d stocks -f /docker-entrypoint-initdb.d/init.sql
   ```

### Running the Pipeline

**Terminal 1 - Start Kafka Producer:**
```bash
python kafka/producer.py
```

**Terminal 2 - Start Spark Streaming:**
```bash
spark-submit \
  --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0 \
  spark/spark_streaming.py
```

**Terminal 3 - Launch Dashboard:**
```bash
streamlit run dashboard/app.py
```

Visit `http://localhost:8501` to see real-time analytics! ğŸ“Š

## ğŸ“Š Features in Detail

### Stock Data Generator
- Simulates realistic stock prices with random walks
- Configurable symbols, frequency, and volatility
- Timestamps with microsecond precision
- Volume and market cap metadata

### Stream Processing
- **Windowed Aggregations**: 1-minute, 5-minute, 15-minute windows
- **Stateful Operations**: Running averages, min/max tracking
- **Watermarking**: Handles late-arriving data
- **Checkpointing**: Fault-tolerant state management

### Analytics
- Real-time price trends
- Volume-weighted average price (VWAP)
- Price change percentages
- Moving averages (SMA, EMA)

## ğŸ§ª Testing

Run unit tests:
```bash
pytest tests/
```

Test Kafka producer independently:
```bash
python kafka/consumer_test.py
```

## ğŸ³ Docker Services

| Service | Port | Description |
|---------|------|-------------|
| Zookeeper | 2181 | Kafka coordination |
| Kafka | 9092 | Message broker |
| PostgreSQL | 5432 | Database (user: `postgres`, db: `stocks`) |
| Adminer | 8080 | Database UI |

Access Adminer at `http://localhost:8080` for database management.

## ğŸ”§ Configuration

Key configuration files:

- `kafka/config.py` - Kafka broker settings, topics
- `spark/config.py` - Spark session config, checkpoints
- `podman-compose.yml` - Infrastructure setup
- `.env` - Environment variables (create from `.env.example`)

## ğŸ“ˆ Monitoring & Observability

**Logs:**
```bash
# Kafka logs
podman-compose logs -f kafka

# View all services
podman-compose logs -f
```

**Metrics** (coming soon):
- Kafka message throughput
- Spark processing latency
- Consumer lag monitoring

## ğŸ› ï¸ Troubleshooting

**Kafka connection refused:**
```bash
# Ensure Kafka is running
podman-compose ps kafka
# Restart if needed
podman-compose restart kafka
```

**Spark can't find packages:**
```bash
# Include Kafka package explicitly
spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0 spark/spark_streaming.py
```

**Database connection errors:**
```bash
# Check PostgreSQL status
podman-compose exec postgres pg_isready
```

## ğŸš€ Future Enhancements

- [ ] ğŸ” **Schema Registry**: Integrate Confluent Schema Registry for schema evolution
- [ ] ğŸ§  **ML Integration**: Anomaly detection with streaming ML models
- [ ] â˜ï¸ **Cloud Deployment**: AWS (MSK + EMR) or GCP (Pub/Sub + Dataflow)
- [ ] ğŸ“Š **Advanced Dashboards**: Grafana with Prometheus metrics
- [ ] ğŸ”’ **Security**: SASL/SSL authentication for Kafka
- [ ] ğŸ“¦ **Kubernetes**: Helm charts for K8s deployment
- [ ] ğŸ§ª **Integration Tests**: End-to-end pipeline testing
- [ ] ğŸ“ **Data Catalog**: Metadata management with Apache Atlas
- [ ] ğŸŒŠ **CDC Integration**: Change data capture from transactional databases

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.


## ğŸ™ Acknowledgments

- Apache Software Foundation for Kafka and Spark
- The open-source community for excellent documentation
- Stock market APIs for inspiration

---

â­ **Star this repo** if you found it helpful!  
ğŸ› **Found a bug?** [Open an issue](https://github.com/your-username/real-time-stock-streaming/issues)  
ğŸ’¡ **Have ideas?** [Start a discussion](https://github.com/your-username/real-time-stock-streaming/discussions)

