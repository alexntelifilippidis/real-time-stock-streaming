# ğŸ“ˆ Real-Time Stock Streaming Pipeline

[![Python](https://img.shields.io/badge/Python-3.8%2B-blue.svg)](https://www.python.org/)
[![Apache Kafka](https://img.shields.io/badge/Apache%20Kafka-3.0%2B-black.svg)](https://kafka.apache.org/)
[![Apache Spark](https://img.shields.io/badge/Apache%20Spark-3.3%2B-orange.svg)](https://spark.apache.org/)
[![Podman](https://img.shields.io/badge/Podman-4.0%2B-892CA0.svg)](https://www.podman.io/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

A production-ready, real-time data streaming pipeline that demonstrates end-to-end data engineering using **Apache Kafka** and **Apache Spark Structured Streaming**. This project simulates live stock market data, processes it in real-time with Pydantic validation, and provides beautiful colored logging.

## ğŸ¯ Key Features

- âš¡ **Real-time Processing**: Sub-second latency streaming with Kafka and Spark
- ğŸ¨ **Beautiful Logging**: Colored, emoji-rich logs with custom formatter
- âœ… **Pydantic Validation**: Type-safe data models with automatic validation
- ğŸ“Š **Scalable Architecture**: Horizontally scalable components using Podman
- ğŸ”„ **Fault Tolerance**: Automatic recovery and checkpointing
- ğŸ³ **Easy Deployment**: Full Podman Compose setup for local development
- ğŸ“ **Production Ready**: Comprehensive logging, monitoring, and error handling

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Data Generator â”‚ â”€â”€â”€â–¶ â”‚  Apache Kafka   â”‚ â”€â”€â”€â–¶ â”‚  Spark Stream   â”‚
â”‚  (Pydantic)     â”‚      â”‚  (5 Partitions) â”‚      â”‚  (PySpark)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                        â”‚                         â”‚
         â”‚                        â”‚                         â”‚
    Validates              Key-based                   Real-time
    with Schema          Partitioning                Aggregations
```

**Data Flow:**
1. ğŸ² **Producer** generates realistic stock price data with Pydantic validation
2. ğŸ“¨ **Kafka** ingests messages with stock symbol as partition key (ordering guarantee)
3. âš¡ **Spark Structured Streaming** consumes and processes data with windowed aggregations
4. ğŸ“Š **Kafka UI** visualizes topics, partitions, and offsets in real-time

## ğŸ› ï¸ Tech Stack

| Layer | Technology | Purpose |
|-------|-----------|---------|
| ğŸ”„ **Streaming** | Apache Kafka (KRaft mode) | High-throughput message broker |
| âš¡ **Processing** | Apache Spark (Structured Streaming) | Real-time stream processing |
| ğŸ **Language** | Python 3.8+ | Main implementation language |
| âœ… **Validation** | Pydantic 2.0+ | Data modeling and validation |
| ğŸ“¦ **Libraries** | kafka-python, PySpark | Data processing |
| ğŸ”§ **Package Manager** | uv | Fast Python package manager |
| ğŸ³ **Infrastructure** | Podman Compose | Containerization |
| ğŸ¨ **Logging** | Custom ColoredFormatter | Beautiful terminal output |

## ğŸ“ Project Structure

```
real-time-stock-streaming/
â”‚
â”œâ”€â”€ README.md                      # This file
â”œâ”€â”€ LICENSE                        # MIT License
â”œâ”€â”€ .gitignore                     # Git ignore rules
â”œâ”€â”€ docker-compose.yml             # Podman services configuration
â”œâ”€â”€ pyproject.toml                 # Python dependencies (uv)
â”œâ”€â”€ Makefile                       # Convenient commands
â”‚
â”œâ”€â”€ src/                           # Source code
â”‚   â”œâ”€â”€ main.py                    # Main entry point
â”‚   â”‚
â”‚   â”œâ”€â”€ kafka/                     # Kafka components
â”‚   â”‚   â”œâ”€â”€ model.py               # Pydantic data models (StockRecord)
â”‚   â”‚   â”œâ”€â”€ producer.py            # Stock data producer with logging
â”‚   â”‚   â”œâ”€â”€ topic_manager.py       # Topic management (create/describe/delete)
â”‚   â”‚   â””â”€â”€ demo_partitions.py     # Demo script for partitions/offsets
â”‚   â”‚
â”‚   â”œâ”€â”€ logger/                    # Custom logging system
â”‚   â”‚   â”œâ”€â”€ __init__.py            # Package exports
â”‚   â”‚   â”œâ”€â”€ models.py              # Pydantic models for logger config
â”‚   â”‚   â””â”€â”€ logger.py              # ColoredFormatter and KafkaModelLogger
â”‚   â”‚
â”‚   â”œâ”€â”€ spark/                     # Spark streaming jobs
â”‚   â”‚   â””â”€â”€ (coming soon)          # Spark streaming application
â”‚   â”‚
â”‚   â””â”€â”€ data/                      # Data directory
â”‚       â””â”€â”€ .gitkeep               # Placeholder
â”‚
â”œâ”€â”€ docs/                          # Documentation
â”‚   â””â”€â”€ KAFKA_PARTITIONS_OFFSETS.md  # Partition/offset guide
â”‚
â””â”€â”€ .github/                       # GitHub templates
    â””â”€â”€ pull_request_template.md  # PR template
```

## ğŸš€ Quick Start

### Prerequisites

- **Podman** 4.0+ and **podman-compose** 2.0+
- **Python** 3.8 or higher
- **uv** package manager (`brew install uv` or `pip install uv`)
- At least 4GB RAM available for containers

### Installation

1. **Clone the repository**
   ```bash
   git clone https://github.com/your-username/real-time-stock-streaming.git
   cd real-time-stock-streaming
   ```

2. **Install dependencies with uv**
   ```bash
   make install
   # or manually: uv sync
   ```

3. **Start the infrastructure** (Kafka + Kafka UI)
   ```bash
   make up
   ```
   
   Wait for services to be healthy (~30 seconds). Check status:
   ```bash
   make status
   ```

4. **Create Kafka topic with 5 partitions**
   ```bash
   make topic-create PARTITIONS=5
   ```

### Running the Pipeline

**Start Kafka Producer:**
```bash
make producer
# or: python src/kafka/producer.py
```

You'll see beautiful colored logs:
```
2025-11-02 10:50:08 - StockProducer - âœ¨ INFO - Connecting to Kafka at localhost:9092
2025-11-02 10:50:08 - StockProducer - âœ¨ INFO - âœ… Successfully connected to Kafka broker
2025-11-02 10:50:08 - StockProducer - âœ¨ INFO - ğŸš€ Starting stock data stream (interval=1.0s)
2025-11-02 10:50:09 - StockProducer - âœ¨ INFO - âœ… AAPL   | $150.23 | Vol:  5,234 | ğŸ“ Partition: 1 | ğŸ“Œ Offset: 42
2025-11-02 10:50:10 - StockProducer - âœ¨ INFO - âœ… GOOGL  | $140.75 | Vol:  8,192 | ğŸ“ Partition: 2 | ğŸ“Œ Offset: 38
```

**View Kafka UI:**
- Open `http://localhost:8080` in your browser
- Navigate to **Topics** â†’ **stock_prices**
- See messages, partitions, and offsets in real-time!

## ğŸ“Š Features in Detail

### Stock Data Producer (`src/kafka/producer.py`)
- âœ… **Pydantic Validation**: Every message validated with `StockRecord` model
- ğŸ¨ **Beautiful Logging**: Colored output with emojis for success/error
- ğŸ”‘ **Partition Keys**: Uses stock symbol as key for consistent partitioning
- ğŸ“Š **Statistics**: Progress tracking every 10 messages
- ğŸ›‘ **Graceful Shutdown**: Ctrl+C handling with proper cleanup
- ğŸ”„ **Context Manager**: Clean resource management

### Pydantic Models (`src/kafka/model.py`)
```python
class StockRecord(BaseModel):
    symbol: Literal["AAPL", "GOOGL", "AMZN", "MSFT", "TSLA", "META", "NFLX"]
    price: float  # Must be > 0
    volume: int   # Must be >= 0
    timestamp: float  # Auto-generated Unix timestamp
```

### Custom Logger (`src/logger/`)
- ğŸ¨ **ANSI Colors**: Beautiful terminal output with color coding
- ğŸ˜€ **Emojis**: Visual indicators for different log levels
- ğŸ—ï¸ **Modular Design**: Separate models and logger implementation
- ğŸ”’ **Frozen Models**: Immutable configuration with Pydantic

**Log Levels:**
- ğŸ” DEBUG (cyan)
- âœ¨ INFO (green)
- âš ï¸ WARNING (yellow)
- âŒ ERROR (red)
- ğŸš¨ CRITICAL (magenta)

## ğŸ§ª Testing & Tools

### Topic Management
```bash
# Create topic with custom partitions
make topic-create PARTITIONS=10

# Describe topic (shows partition and offset info)
make topic-describe

# List all topics
make topic-list

# Delete topic
make topic-delete
```

### Demo Scripts
```bash
# Run partition/offset demonstration
python src/kafka/demo_partitions.py
```

## ğŸ³ Podman Services

| Service | Port | Description |
|---------|------|-------------|
| Kafka | 9092 | Message broker (KRaft mode, 5 partitions) |
| Kafka UI | 8080 | Web UI for Kafka management |

Access Kafka UI at `http://localhost:8080` to:
- View topics and messages
- See partition distribution
- Track consumer offsets
- Monitor cluster health

## ğŸ”§ Makefile Commands

```bash
make help           # Show all available commands
make install        # Install Python dependencies
make dev-install    # Install with dev dependencies (pytest, ruff, mypy)
make up             # Start Podman services
make down           # Stop services
make restart        # Restart services
make nuke           # Destroy all containers/images (âš ï¸  destructive!)
make producer       # Run Kafka producer
make topic-create   # Create Kafka topic
make topic-describe # Show partition info
make clean          # Clean Python cache files
make status         # Check service status
```

## ğŸ“š Understanding Kafka Partitions & Offsets

**Partitions**: Like highway lanes - messages are distributed across them for parallel processing.
- Your producer uses **stock symbol as partition key**
- This ensures all AAPL messages go to the same partition
- Provides **ordering guarantee** per symbol

**Offsets**: Sequential IDs for messages within each partition
- Start at 0 and increment
- Act as bookmarks for tracking position
- Enable fault-tolerant consumption

See `docs/KAFKA_PARTITIONS_OFFSETS.md` for detailed explanation.

## ğŸ› ï¸ Troubleshooting

**Podman machine not running:**
```bash
make up  # Automatically starts Podman machine
```

**Kafka connection refused:**
```bash
# Check Kafka status
make status
# Restart Kafka
make restart
```

**Import errors:**
```bash
# Ensure dependencies are installed
uv sync
```

**View logs:**
```bash
# All services
podman-compose logs -f
# Just Kafka
podman-compose logs -f kafka
```

## ğŸš€ Future Enhancements

- [ ] âš¡ **Spark Streaming**: Real-time aggregations and windowing
- [ ] ğŸ’¾ **PostgreSQL**: Store processed results
- [ ] ğŸ“Š **Streamlit Dashboard**: Live visualization
- [ ] ğŸ” **Schema Registry**: Schema evolution support
- [ ] ğŸ§  **ML Integration**: Anomaly detection
- [ ] â˜ï¸ **Cloud Deployment**: AWS MSK or GCP Pub/Sub
- [ ] ğŸ”’ **Security**: SASL/SSL authentication
- [ ] ğŸ“ˆ **Monitoring**: Prometheus + Grafana

## ğŸ¤ Contributing

Contributions are welcome! Please:

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

Use the PR template in `.github/pull_request_template.md`

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- Apache Software Foundation for Kafka and Spark
- Pydantic team for excellent data validation
- The open-source community

---

â­ **Star this repo** if you found it helpful!  
ğŸ› **Found a bug?** [Open an issue](https://github.com/your-username/real-time-stock-streaming/issues)  
ğŸ’¡ **Have ideas?** [Start a discussion](https://github.com/your-username/real-time-stock-streaming/discussions)

**Made with â¤ï¸ and â˜• by Data Engineers, for Data Engineers**

